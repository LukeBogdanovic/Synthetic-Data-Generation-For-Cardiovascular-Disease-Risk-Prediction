{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 00:33:13.376605: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-17 00:33:13.466592: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-17 00:33:13.492899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-17 00:33:13.658325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-17 00:33:14.972921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.api.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gan_pretrain_preprocessing import reverse_ecg_normalization, normalize_ecg\n",
    "from keras.api.layers import Bidirectional, TimeDistributed, LSTM, Dense, Flatten, Conv1D, Reshape, Dropout, LeakyReLU, Layer\n",
    "from keras.api.metrics import Mean\n",
    "from scipy.spatial.distance import euclidean\n",
    "from keras.api.callbacks import Callback\n",
    "from keras.api.saving import register_keras_serializable, load_model\n",
    "import ctypes\n",
    "\n",
    "dtw_lib = ctypes.CDLL(\"c_funcs/dtw.so\")\n",
    "dtw_lib.dtw_distance.argtypes = [ctypes.POINTER(\n",
    "    ctypes.c_double), ctypes.POINTER(ctypes.c_double), ctypes.c_int, ctypes.c_int]\n",
    "dtw_lib.dtw_distance.restype = ctypes.c_double\n",
    "dtw_lib.compute_mmd.argtypes = [ctypes.POINTER(ctypes.c_double),\n",
    "                                ctypes.POINTER(ctypes.c_double),\n",
    "                                ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_double]\n",
    "dtw_lib.compute_mmd.restype = ctypes.c_double\n",
    "\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_unconditional(ecg_length=128, n_leads=3, latent_dim=100) -> Model:\n",
    "    noise_input = Input(shape=(latent_dim,), name='Noise_input')\n",
    "    x = Dense(ecg_length * 32, activation='relu')(noise_input)\n",
    "    x = Reshape((ecg_length, 32))(x)\n",
    "    x = Conv1D(64, kernel_size=5, strides=1,\n",
    "               padding='same', activation='relu')(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
    "    out = TimeDistributed(Dense(n_leads, activation='tanh'))(x)\n",
    "    generator = Model(\n",
    "        inputs=noise_input,\n",
    "        outputs=out,\n",
    "        name='Generator'\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "\n",
    "def build_critic(ecg_length=128, n_leads=3) -> Model:\n",
    "    ecg_input = Input(shape=(ecg_length, n_leads), name='ecg_input')\n",
    "    x = Conv1D(64, kernel_size=3, strides=2, padding='same')(ecg_input)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(128, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(256, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(negative_slope=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = MiniBatchDiscrimination(num_kernel=100, dim_kernel=5)(x)\n",
    "    out = Dense(1)(x)\n",
    "    critic = Model(\n",
    "        inputs=ecg_input,\n",
    "        outputs=out,\n",
    "        name='Discriminator'\n",
    "    )\n",
    "    return critic\n",
    "\n",
    "\n",
    "def compute_mmd(real_ecg, fake_ecg, sigma=1.0):\n",
    "    if isinstance(real_ecg, tf.Tensor):\n",
    "        real_ecg = tf.convert_to_tensor(real_ecg)\n",
    "        real_ecg = real_ecg.numpy()\n",
    "    if isinstance(fake_ecg, tf.Tensor):\n",
    "        fake_ecg = tf.convert_to_tensor(fake_ecg)\n",
    "        fake_ecg = fake_ecg.numpy()\n",
    "    real_np = real_ecg.reshape(real_ecg.shape[0], -1).astype(np.float64)\n",
    "    fake_np = fake_ecg.reshape(fake_ecg.shape[0], -1).astype(np.float64)\n",
    "    batch_real, features = real_np.shape\n",
    "    batch_fake, _ = fake_np.shape\n",
    "    result = dtw_lib.compute_mmd(\n",
    "        real_np.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n",
    "        fake_np.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n",
    "        batch_real, batch_fake, features, sigma\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_mvdTW(real_ecg, fake_ecg):\n",
    "    real_np = real_ecg.numpy() if isinstance(real_ecg, tf.Tensor) else real_ecg\n",
    "    fake_np = fake_ecg.numpy() if isinstance(fake_ecg, tf.Tensor) else fake_ecg\n",
    "    batch_size = real_np.shape[0]\n",
    "    ecg_length = real_np.shape[1]\n",
    "    n_leads = real_np.shape[2]\n",
    "    distances = []\n",
    "    for i in range(batch_size):\n",
    "        seq1 = real_np[i].flatten().astype(np.float64)\n",
    "        seq2 = fake_np[i].flatten().astype(np.float64)\n",
    "        dtw_distance_val = dtw_lib.dtw_distance(\n",
    "            seq1.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n",
    "            seq2.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n",
    "            ecg_length,\n",
    "            ecg_length,\n",
    "            n_leads\n",
    "        )\n",
    "        distances.append(dtw_distance_val)\n",
    "    return np.mean(distances).astype(np.float32)\n",
    "\n",
    "\n",
    "def mvDTW_loss(real_ecgs, fake_ecgs):\n",
    "    return tf.numpy_function(compute_mvdTW, [real_ecgs, fake_ecgs], tf.float32)\n",
    "\n",
    "\n",
    "@register_keras_serializable(package='Custom')\n",
    "class WGANGP(Model):\n",
    "    def __init__(self, generator: Model, critic: Model, latent_dim=100, n_critic=5, lambda_gp=10.0, lambda_dtw=0.5, **kwargs):\n",
    "        super(WGANGP, self).__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.critic = critic\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_critic = n_critic\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.lambda_dtw = lambda_dtw\n",
    "        self.mmd_metric = Mean(name=\"mmd\")\n",
    "        # Multivariate dynamic time warping\n",
    "        self.mvdTW_metric = Mean(name=\"mvdTW\")\n",
    "\n",
    "    def generate(self, noise):\n",
    "        '''\n",
    "        Function for large quantity generation of Multivariate ECG signals.\\\\\n",
    "        Quantity should be greater than the batch size set for the during training model.\n",
    "\n",
    "        :param:\n",
    "        noise\n",
    "\n",
    "        :return:\n",
    "\n",
    "        '''\n",
    "        return self.generator.predict(noise)\n",
    "\n",
    "    def discriminate(self, ecg):\n",
    "        '''\n",
    "        Function for checking validity of validity of large quantity of Multivariate ECG signals.\\\\\n",
    "        Quantity should be greater than the batch size set for the model during training.\n",
    "        '''\n",
    "        return self.critic.predict(ecg)\n",
    "\n",
    "    def call_generate(self, noise, training=False):\n",
    "        '''\n",
    "        Function for creating single sample of Multivariate ECG signals.\n",
    "        '''\n",
    "        return self.generator(noise, training=training)\n",
    "\n",
    "    def call_discriminate(self, ecg, training=False):\n",
    "        '''\n",
    "        Function for checking validity of single sample of Multivariate ECG signals.\n",
    "        '''\n",
    "        return self.critic(ecg, training=training)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.mmd_metric, self.mvdTW_metric]\n",
    "\n",
    "    def compile(self, g_optimizer: Adam, c_optimizer: Adam, **kwargs):\n",
    "        super(WGANGP, self).compile(**kwargs)\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.c_optimizer = c_optimizer\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(WGANGP, self).get_config()\n",
    "        config.update({\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"n_critic\": self.n_critic,\n",
    "            \"lambda_gp\": self.lambda_gp,\n",
    "            \"lambda_dtw\": self.lambda_dtw\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        Loads the model from the saved configuration.\n",
    "        \"\"\"\n",
    "        generator = config.pop(\"generator\")\n",
    "        critic = config.pop(\"critic\")\n",
    "        return cls(generator=generator, critic=critic, **config)\n",
    "\n",
    "    def gradient_penalty(self, real_samples, fake_samples):\n",
    "        batch_size = tf.shape(real_samples)[0]\n",
    "        epsilon = tf.random.uniform(\n",
    "            [batch_size, 1, 1], 0.0, 1.0, dtype=tf.float16)\n",
    "        interpolated = epsilon * real_samples + (1 - epsilon) * fake_samples\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            interpolated_score = self.critic(interpolated, training=True)\n",
    "        grads = tape.gradient(interpolated_score, [interpolated])[0]\n",
    "        grads = tf.reshape(grads, [batch_size, -1])\n",
    "        grad_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1) + 1e-12)\n",
    "        penalty = tf.reduce_mean((grad_norm - 1.0) ** 2)\n",
    "        return penalty\n",
    "\n",
    "    def train_step(self, real_ecg):\n",
    "        batch_size = tf.shape(real_ecg)[0]\n",
    "        for _ in range(self.n_critic):\n",
    "            noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_ecg = self.generator(noise, training=True)\n",
    "                total_critic_loss = self.critic_loss(real_ecg, fake_ecg)\n",
    "            critic_gradients = tape.gradient(\n",
    "                total_critic_loss, self.critic.trainable_variables)\n",
    "            self.c_optimizer.apply_gradients(\n",
    "                zip(critic_gradients, self.critic.trainable_variables))\n",
    "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_ecg = self.generator(noise, training=True)\n",
    "            g_loss = self.generator_loss(real_ecg, fake_ecg)\n",
    "        generator_gradients = tape.gradient(\n",
    "            g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, self.generator.trainable_variables))\n",
    "        mmd_value = tf.py_function(func=compute_mmd, inp=[\n",
    "                                   real_ecg, fake_ecg], Tout=tf.float32)\n",
    "        mvdTW_value = tf.py_function(func=compute_mvdTW, inp=[\n",
    "                                     real_ecg, fake_ecg], Tout=tf.float32)\n",
    "        mmd_value.set_shape([])\n",
    "        mvdTW_value.set_shape([])\n",
    "        self.mmd_metric.update_state(mmd_value)\n",
    "        self.mvdTW_metric.update_state(mvdTW_value)\n",
    "        return {\"critic_loss\": total_critic_loss, \"generator_loss\": g_loss, \"mmd\": self.mmd_metric.result(), \"mvDTW\": self.mvdTW_metric.result()}\n",
    "\n",
    "    def generator_loss(self, real_ecgs, fake_ecgs):\n",
    "        critic_fake = self.critic(fake_ecgs, training=True)\n",
    "        g_loss = -tf.reduce_mean(critic_fake)\n",
    "        dtw_loss = mvDTW_loss(real_ecgs, fake_ecgs)\n",
    "        return tf.cast(g_loss, tf.float32) + self.lambda_dtw * dtw_loss\n",
    "\n",
    "    def critic_loss(self, real_ecg, fake_ecg):\n",
    "        critic_real = self.critic(real_ecg, training=True)\n",
    "        critic_fake = self.critic(fake_ecg, training=True)\n",
    "        critic_loss = tf.reduce_mean(\n",
    "            critic_fake) - tf.reduce_mean(critic_real)\n",
    "        gp = self.gradient_penalty(real_ecg, fake_ecg)\n",
    "        return critic_loss + self.lambda_gp * gp\n",
    "\n",
    "\n",
    "@register_keras_serializable(package='Custom')\n",
    "class MiniBatchDiscrimination(Layer):\n",
    "    def __init__(self, num_kernel, dim_kernel, kernel_initalizer='glorot_uniform', **kwargs):\n",
    "        self.num_kernel = num_kernel\n",
    "        self.dim_kernel = dim_kernel\n",
    "        self.kernel_initializer = kernel_initalizer\n",
    "        super(MiniBatchDiscrimination, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=(input_shape[1], self.num_kernel * self.dim_kernel),\n",
    "            initializer=self.kernel_initializer,\n",
    "            trainable=True\n",
    "        )\n",
    "        super(MiniBatchDiscrimination, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        activation = tf.matmul(x, self.kernel)\n",
    "        activation = tf.reshape(\n",
    "            activation, shape=(-1, self.num_kernel, self.dim_kernel))\n",
    "        tmp1 = tf.expand_dims(activation, 3)\n",
    "        tmp2 = tf.transpose(activation, perm=[1, 2, 0])\n",
    "        tmp2 = tf.expand_dims(tmp2, 0)\n",
    "        diff = tmp1 - tmp2\n",
    "        l1 = tf.reduce_sum(tf.math.abs(diff), axis=2)\n",
    "        features = tf.reduce_sum(tf.math.exp(-l1), axis=2)\n",
    "        return tf.concat([x, features], axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1] + self.num_kernel)\n",
    "\n",
    "\n",
    "class SaveGeneratedECG(Callback):\n",
    "    def __init__(self, generator, save_path=\"images/generated_images\"):\n",
    "        super(SaveGeneratedECG, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.save_path = save_path\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "        noise = np.random.normal(0, 1, (1, 50))\n",
    "        gen_ecg = self.generator.predict(\n",
    "            noise, verbose=0).squeeze()\n",
    "        gen_ecg = reverse_ecg_normalization(gen_ecg, m_scaler)\n",
    "        if gen_ecg.ndim == 1:\n",
    "            gen_ecg = gen_ecg[:, np.newaxis]\n",
    "        num_leads = gen_ecg.shape[1]\n",
    "        for lead in range(num_leads):\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(gen_ecg[:, lead], linewidth=1.5, color='black')\n",
    "            plt.xlabel(\"Time (samples)\")\n",
    "            plt.ylabel(\"Amplitude\")\n",
    "            plt.title(f\"Generated ECG - Lead {lead+1} - Epoch {epoch + 1}\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(\n",
    "                f\"{self.save_path}/ecg_epoch_{epoch + 1}_lead_{lead + 1}.png\", bbox_inches='tight', dpi=300)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elukbog/.local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:730: UserWarning: Model 'wgangp' had a build config, but the model cannot be built automatically in `build_from_config(config)`. You should implement `def build_from_config(self, config)`, and you might also want to implement the method  that generates the config at saving time, `def get_build_config(self)`. The method `build_from_config()` is meant to create the state of the model (i.e. its variables) upon deserialization.\n",
      "  instance.build_from_config(build_config)\n",
      "/home/elukbog/.local/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:734: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
      "  instance.compile_from_config(compile_config)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'TrackedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# Should return True if \"generator\" exists\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m gen_ecgs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Concatenate the generated segments\u001b[39;00m\n\u001b[1;32m     20\u001b[0m gen_ecgs_full \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(gen_ecgs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 122\u001b[0m, in \u001b[0;36mWGANGP.call_generate\u001b[0;34m(self, noise, training)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, noise, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    Function for creating single sample of Multivariate ECG signals.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TrackedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "data = np.load(\"../normalized_ecg.npy\", allow_pickle=True)\n",
    "n_records = len(data)\n",
    "subset_size = 10000\n",
    "indices_all = np.arange(n_records)\n",
    "indices = np.random.choice(indices_all, subset_size, replace=False)\n",
    "subset_ecg_dataset = [data[i] for i in indices]\n",
    "m_scaler = MinMaxScaler(feature_range=(-1, 1)\n",
    "                        ).fit(np.vstack(subset_ecg_dataset))\n",
    "\n",
    "# _, m_scaler, c_bin_minmax_scaler = load_data(segment_length=1)\n",
    "# seconds_to_generate = 60\n",
    "Leads = ['III', 'V3', 'V5']\n",
    "\n",
    "model: Model = load_model(f\"gan/wgan.keras\", custom_objects={\"WGANGP\": WGANGP})\n",
    "noise = np.random.normal(0, 1, (1, 50))\n",
    "print(hasattr(model, \"generator\"))  # Should return True if \"generator\" exists\n",
    "gen_ecgs = model.call_generate(noise)\n",
    "\n",
    "# Concatenate the generated segments\n",
    "gen_ecgs_full = np.concatenate(gen_ecgs, axis=0)\n",
    "gen_ecgs_full = reverse_ecg_normalization(gen_ecgs_full, m_scaler)\n",
    "# gen_crfs = [gen_crfs_g[0][0], gen_crfs_n[0][0], gen_crfs_n[0][1], gen_crfs_n[0]\n",
    "#             [2], gen_crfs_s[0][0], gen_crfs_n[0][3], gen_crfs_n[0][4], gen_crfs_v[0][0]]\n",
    "# gen_crfs_full = reverse_crf_to_df(gen_crfs, scaler, c_bin_minmax_scaler, col_names=[\n",
    "#     'Gender', 'Age', 'Weight', 'Height', 'Smoker', 'SBP', 'DBP', 'Vascular Event'])\n",
    "# print(gen_crfs_full)\n",
    "plt.figure(0, figsize=(15, 10))\n",
    "for lead_idx in range(gen_ecgs_full.shape[1]):\n",
    "    plt.subplot(3, 1, lead_idx + 1)\n",
    "    plt.plot(gen_ecgs_full[:, lead_idx], label=f'Lead {Leads[lead_idx]}')\n",
    "    plt.title(f'Fake ECG - Lead {Leads[lead_idx]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
